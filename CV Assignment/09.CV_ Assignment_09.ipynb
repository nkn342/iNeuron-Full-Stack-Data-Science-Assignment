{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0679dbb5",
   "metadata": {},
   "source": [
    "# Assignment 9 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f5d3ab",
   "metadata": {},
   "source": [
    "#### Q1. What are the advantages of a CNN for image classification over a completely linked DNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e33d7",
   "metadata": {},
   "source": [
    "*Ans**: Convolutional Neural Networks (CNNs) are specifically designed for image classification tasks and offer several advantages over fully connected Deep Neural Networks (DNNs), which are more general-purpose models.\n",
    "\n",
    "**Here are some advantages of CNNs over DNNs for image classification:**\n",
    "\n",
    "* **Spatial Hierarchies**: CNNs can capture spatial hierarchies by learning local patterns in an image and combining them to form higher-level representations. This is achieved through the use of convolutional layers, which apply filters to different parts of the image and learn to recognize specific features.\n",
    "\n",
    "* **Parameter Efficiency**: CNNs are more parameter-efficient than DNNs, as they reuse the same set of weights across different parts of an image. This allows for faster training times and reduces the risk of overfitting.\n",
    "\n",
    "* **Translation Invariance:** CNNs are able to recognize objects in images regardless of their position or orientation, due to their ability to use the same filters across different parts of the image. This makes them more robust to variations in input data.\n",
    "\n",
    "* **Feature Extraction**: CNNs can automatically learn and extract features from raw image data, which is not possible with DNNs. This reduces the need for manual feature engineering, which can be time-consuming and difficult.\n",
    "\n",
    "Overall, CNNs are better suited for image classification tasks due to their ability to capture spatial hierarchies, parameter efficiency, translation invariance, and feature extraction capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9e1c1",
   "metadata": {},
   "source": [
    "#### Q2. Consider a CNN with three convolutional layers, each of which has three kernels, a stride of two, and SAME padding. The bottom layer generates 100 function maps, the middle layer 200, and the top layer 400. RGB images with a size of 200 x 300 pixels are used as input. How many criteria does the CNN have in total? How much RAM would this network need when making a single instance prediction if we're using 32-bit floats? What if you were to practice on a batch of 50 images?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f842e7",
   "metadata": {},
   "source": [
    "**Ans**: To calculate the total number of parameters in the CNN, we need to compute the number of parameters in each layer and then add them up.\n",
    "\n",
    "For each convolutional layer, the number of parameters can be calculated as follows:\n",
    "\n",
    "  * **Number of parameters in each kernel** = kernel size * number of input feature maps * number of output feature maps\n",
    "  * **Number of bias parameters** = number of output feature maps\n",
    "\n",
    "The kernel size is not given, so let's assume it to be 3x3.\n",
    "\n",
    "**For the bottom layer:**\n",
    "\n",
    "  * **Number of input feature maps** = 3 (RGB channels)\n",
    "  * **Number of output feature maps** = 100\n",
    "  * **Number of parameters in each kernel** = 3 * 3 * 3 * 100 = 2700\n",
    "  * **Number of bias parameters** = 100\n",
    "  * **Total number of parameters in the bottom layer** = (2700 + 100) * 3 = 8400\n",
    "\n",
    "**For the middle layer:**\n",
    "\n",
    "  * **Number of input feature maps** = 100 (output feature maps of the bottom layer)\n",
    "  * **Number of output feature maps** = 200\n",
    "  * **Number of parameters in each kernel** = 3 * 3 * 100 * 200 = 180,000\n",
    "  * **Number of bias parameters** = 200\n",
    "  * **Total number of parameters in the middle layer** = (180000 + 200)*3 = 540600\n",
    "\n",
    "**For the top layer:**\n",
    "\n",
    "  * **Number of input feature maps** = 200 (output feature maps of the middle layer)\n",
    "  * **Number of output feature maps** = 400\n",
    "  * **Number of parameters in each kernel** = 3 * 3 * 200 * 400 = 1,440,000\n",
    "  * **Number of bias parameters** = 400\n",
    "  * **Total number of parameters in the top layer** = (1440000 + 400) * 3 = 4,320,400\n",
    "\n",
    "Therefore, the total number of parameters in the CNN is the sum of the number of parameters in each layer, which is:\n",
    "\n",
    "**8400 + 540600 + 4320400 = 4,863,400 parameters**\n",
    "\n",
    "To calculate the amount of RAM required for one instance prediction, we need to calculate the amount of memory required to store the input and output feature maps and the parameters. Assuming we are using 32-bit floats, the amount of memory required to store one instance prediction is:\n",
    "\n",
    "  * **Input image**: 200 x 300 x 3 x 4 bytes = 720,000 bytes\n",
    "\n",
    "2Output feature maps of bottom layer: 100 x 100 x 150 x 4 bytes = 60,000,000 bytes\n",
    "\n",
    "  * **Output feature maps of middle layer**: 200 x 50 x 75 x 4 bytes = 30,000,000 bytes\n",
    "  * **Output feature maps of top layer**: 400 x 25 x 38 x 4 bytes = 19,200,000 bytes\n",
    "  * **Parameters**: 4,863,400 x 4 bytes = 19,453,600 bytes\n",
    "  * **Total memory required for one instance prediction**: 99,433,600 bytes or approximately 94.8 MB\n",
    "\n",
    "If we were to process a batch of 50 images, the amount of memory required would be:\n",
    "\n",
    "  * **Input images**: 50 x 200 x 300 x 3 x 4 bytes = 180,000,000 bytes\n",
    "  * **Output feature maps of bottom layer**: 50 x 100 x 100 x 150 x 4 bytes = 1,500,000,000 bytes\n",
    "  * **Output feature maps of middle layer**: 50 x 200 x 50 x 75 x 4 bytes = 1,500,000,000 bytes\n",
    "  * **Output feature maps of top layer**: 50 x 400 x 25 x 38 x 4 bytes = 3,840,000,000 bytes\n",
    "  * **Parameters**: 4,863,400 x 4 bytes = 19,453,600 bytes\n",
    "  * **Total memory required for a batch of 50 image predictions**: 6,940,453,600 bytes or approximately 6.94 GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f942ab",
   "metadata": {},
   "source": [
    "#### Q3. What are five things you might do to fix the problem if your GPU runs out of memory while training a CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff9df9",
   "metadata": {},
   "source": [
    "**Ans**: If your GPU runs out of memory while training a Convolutional Neural Network (CNN), here are five things you can do to fix the problem:\n",
    "\n",
    "* **Reduce batch size**: You can reduce the batch size to fit the available memory of the GPU. This approach divides the dataset into smaller batches that can fit in the GPU memory.\n",
    "\n",
    "* **Reduce image size**: You can resize the images to a smaller size before training the CNN. This reduces the memory required to store the images and can help fit the model in the GPU memory.\n",
    "\n",
    "* **Use a smaller model**: If your current CNN model is too large, you can use a smaller model architecture that requires less memory to train.\n",
    "\n",
    "* **Use mixed precision training**: You can use mixed precision training, which trains the model using lower precision data types (e.g., half-precision floating-point numbers) to reduce memory usage.\n",
    "\n",
    "* **Use transfer learning**: You can use transfer learning by starting with a pre-trained model and then fine-tuning it on your dataset. This approach can save a lot of training time and memory usage since the pre-trained model is already optimized for the specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d93d9d",
   "metadata": {},
   "source": [
    "#### Q4. Why would you use a max pooling layer instead with a convolutional layer of the same stride?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9aa4a",
   "metadata": {},
   "source": [
    "**Ans**: Max pooling is a type of pooling operation commonly used in convolutional neural networks (CNNs) for image recognition tasks. The purpose of max pooling is to reduce the spatial size of the feature map and to make the model more robust to variations in the input image.\n",
    "\n",
    "Max pooling works by partitioning the input feature map into a set of non-overlapping rectangular regions, and then taking the maximum value of each region. This reduces the dimensionality of the feature map, while preserving the most salient features.\n",
    "\n",
    "One reason to use max pooling instead of a convolutional layer with the same stride is that max pooling can help reduce overfitting. By discarding some of the less important features in the feature map, max pooling forces the network to learn more robust and generalizable features. This can help prevent the network from memorizing the training data and performing poorly on new, unseen data.\n",
    "\n",
    "Another reason to use max pooling is that it can help reduce the computational cost of the network. Max pooling reduces the number of parameters in the network and can help speed up the training process.\n",
    "\n",
    "In summary, max pooling is a useful tool for reducing the spatial size of feature maps in CNNs. It can help reduce overfitting, improve generalization, and reduce computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136ecf74",
   "metadata": {},
   "source": [
    "#### Q5. When would a local response normalization layer be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24425d5",
   "metadata": {},
   "source": [
    "**Ans**: A local response normalization (LRN) layer is a type of normalization layer used in neural networks, particularly in convolutional neural networks (CNNs). LRN layers apply a normalization function to the output of each neuron in a feature map, based on the responses of neighboring neurons within the same feature map.\n",
    "\n",
    "The main purpose of LRN layers is to enhance the ability of the network to discriminate between different features by reducing the response of neurons that are sensitive to high levels of activation. This can help prevent saturation and improve the discriminative power of the network.\n",
    "\n",
    "LRN layers are most commonly used in image recognition tasks, where they can help improve the accuracy of the network by reducing the impact of irrelevant features in the input image. Specifically, LRN layers are useful in situations where the input images contain a lot of noise or where there is significant variation in the lighting or color of the images.\n",
    "\n",
    "LRN layers can also be useful in deep networks with many layers, where the output of each layer can become highly correlated and lead to overfitting. By normalizing the output of each neuron based on the responses of neighboring neurons within the same feature map, LRN layers can help reduce the correlation between the output of different neurons and improve the generalization ability of the network.\n",
    "\n",
    "Overall, LRN layers can be useful in a wide range of image recognition tasks, particularly in situations where the input images contain a lot of noise or where the network has many layers and is at risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc6d74",
   "metadata": {},
   "source": [
    "#### Q6. In comparison to LeNet-5, what are the main innovations in AlexNet? What about GoogLeNet and ResNet's core innovations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab84d7",
   "metadata": {},
   "source": [
    "**Ans**: LeNet-5, proposed by Yann LeCun et al. in 1998, was one of the first successful convolutional neural networks for image recognition. It consisted of a series of convolutional and pooling layers, followed by fully connected layers.\n",
    "\n",
    "AlexNet, proposed by Alex Krizhevsky et al. in 2012, built upon the ideas of LeNet-5 but introduced several key innovations that helped to significantly improve the performance of convolutional neural networks. Some of the main innovations in AlexNet include:\n",
    "\n",
    "1. **Larger network**: AlexNet was a much larger network than LeNet-5, with more layers and more parameters.\n",
    "\n",
    "2. **Rectified linear units (ReLU)**: AlexNet used ReLU activation functions, which helped to reduce the problem of vanishing gradients and improve the training speed.\n",
    "\n",
    "3. **Local response normalization (LRN)**: AlexNet used LRN layers, which helped to reduce overfitting and improve the generalization ability of the network.\n",
    "\n",
    "4. **Dropout**: AlexNet also used dropout, which helped to further reduce overfitting by randomly dropping out some of the neurons during training.\n",
    "\n",
    "GoogLeNet, proposed by Szegedy et al. in 2014, was a highly innovative network that introduced several new ideas for improving the performance of convolutional neural networks. Some of the main innovations in GoogLeNet include:\n",
    "\n",
    "1. **Inception modules**: GoogLeNet used inception modules, which allowed the network to learn features at different scales and improve the representational capacity of the network.\n",
    "\n",
    "2. **1x1 convolutions**: GoogLeNet also used 1x1 convolutions, which helped to reduce the number of parameters in the network and improve the computational efficiency.\n",
    "\n",
    "3. **Global average pooling**: GoogLeNet used global average pooling, which helped to reduce overfitting and improve the generalization ability of the network.\n",
    "\n",
    "ResNet, proposed by He et al. in 2015, introduced a new architecture for very deep convolutional neural networks that allowed them to be trained much more effectively than previous networks. The main innovation in ResNet was the use of residual connections, which allowed the network to learn residual functions instead of directly learning the desired mapping. This helped to address the problem of vanishing gradients and made it possible to train much deeper networks than was previously possible.\n",
    "\n",
    "Overall, the core innovations in these networks reflect a continuous effort to improve the performance of convolutional neural networks. While LeNet-5 was a breakthrough in its time, subsequent networks such as AlexNet, GoogLeNet, and ResNet have continued to push the boundaries of what is possible in image recognition and other applications of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb15ce",
   "metadata": {},
   "source": [
    "#### Q7. On MNIST, build your own CNN and strive to achieve the best possible accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867a1ae7",
   "metadata": {},
   "source": [
    "**Ans**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f9efc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a412514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c748322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0ff7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43527666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46f29205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1875/1875 [==============================] - 37s 19ms/step - loss: 0.1287 - accuracy: 0.9608 - val_loss: 0.0440 - val_accuracy: 0.9849\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 43s 23ms/step - loss: 0.0422 - accuracy: 0.9866 - val_loss: 0.0337 - val_accuracy: 0.9891\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 33s 18ms/step - loss: 0.0287 - accuracy: 0.9909 - val_loss: 0.0267 - val_accuracy: 0.9900\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.0218 - accuracy: 0.9931 - val_loss: 0.0247 - val_accuracy: 0.9921\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.0162 - accuracy: 0.9948 - val_loss: 0.0290 - val_accuracy: 0.9905\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.0124 - accuracy: 0.9959 - val_loss: 0.0325 - val_accuracy: 0.9904\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 0.0287 - val_accuracy: 0.9921\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.0085 - accuracy: 0.9970 - val_loss: 0.0426 - val_accuracy: 0.9887\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0358 - val_accuracy: 0.9907\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.0358 - val_accuracy: 0.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x299bf6dc970>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train.reshape(-1, 28, 28, 1), y_train, epochs=10, validation_data=(x_test.reshape(-1, 28, 28, 1), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39edd646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0358 - accuracy: 0.9904\n",
      "Test accuracy: 0.9904000163078308\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test.reshape(-1, 28, 28, 1), y_test)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20917aef",
   "metadata": {},
   "source": [
    "#### Q8. Using Inception v3 to classify broad images. a. Images of different animals can be downloaded. Load them in Python using the matplotlib.image.mpimg.imread() or scipy.misc.imread() functions, for example. Resize and/or crop them to 299 x 299 pixels, and make sure they only have three channels (RGB) and no transparency.The photos used to train the Inception model were preprocessed to have values ranging from -1.0 to 1.0, so make sure yours do as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c07ed85",
   "metadata": {},
   "source": [
    "**Ans**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d16dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d59e44f",
   "metadata": {},
   "source": [
    "#### Q9. Large-scale image recognition using transfer learning.\n",
    "\n",
    "a. Make a training set of at least 100 images for each class. You might, for example, identify your own photos based on their position (beach, mountain, area, etc.) or use an existing dataset, such as the flowers dataset or MIT's places dataset (requires registration, and it is huge).\n",
    "\n",
    "b. Create a preprocessing phase that resizes and crops the image to 299 x 299 pixels while also adding some randomness for data augmentation.\n",
    "\n",
    "c. Using the previously trained Inception v3 model, freeze all layers up to the bottleneck layer (the last layer before output layer) and replace output layer with appropriate number of outputs for your new classification task (e.g., the flowers dataset has five mutually exclusive classes so the output layer must have five neurons and use softmax activation function).\n",
    "\n",
    "d. Separate the data into two sets: a training and a test set. The training set is used to train the model, and the test set is used to evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76578a97",
   "metadata": {},
   "source": [
    "**Ans**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55759545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
